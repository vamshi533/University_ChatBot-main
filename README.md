# ğŸ“ SRMAP University ChatBot

A web scraping and semantic search system that collects data, processes it using deep learning embeddings, and provides an interactive interface for exploring the results.

---

## ğŸ“ Description

This project is a comprehensive web scraping and data processing system that follows a sequential workflow:

1. ğŸ•¸ï¸ Web scraping using `deep_scraper.py` to collect data
2. ğŸ§  Processing and embedding the scraped data using `modified_embedder.py`
3. ğŸ—‚ï¸ Generating FAISS index and metadata files
4. ğŸ’¬ Visualizing and interacting with the processed data through a Gradio interface in the Jupyter notebook

---

## ğŸ› ï¸ Core Technologies

- ğŸ§© **Embedding Model:** all-MiniLM-L6-v2
- ğŸ¤– **LLM:** TinyLlama-1.1B
- ğŸ—ƒï¸ **Vector Database:** FAISS

---

## âœ¨ Features

- ğŸŒ Advanced web scraping with BeautifulSoup4 and aiohttp
- ğŸ§  Deep learning-based text embedding using sentence-transformers
- âš¡ Efficient similarity search with FAISS
- ğŸ–¥ï¸ Gradio-based interactive interface
- ğŸ“„ Support for PDF processing with PyMuPDF
- ğŸš€ Asynchronous processing for improved performance

---

## ğŸ“¦ Requirements

- ğŸ Python 3.8+
- ğŸ”¥ PyTorch
- ğŸ¤— Transformers
- ğŸ§  Sentence-transformers
- ğŸ—ƒï¸ FAISS
- ğŸ–¥ï¸ Gradio
- ğŸŒ¸ BeautifulSoup4
- ğŸŒ aiohttp
- ğŸ“„ PyMuPDF
- ğŸ“‹ Other dependencies listed in requirements.txt

---

## âš™ï¸ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/vamshi533/University_ChatBot.git
   cd University_ChatBot
   ```

2. Create a virtual environment (recommended):

   ```bash
   python -m venv venv
   venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ—‚ï¸ Project Structure

- `deep_scraper.py`: Initial web scraping module
- `modified_embedder.py`: Data processing and embedding module
- `University_Chatbot.ipynb`: Jupyter notebook containing the Gradio interface
- `requirements.txt`: Project dependencies

---

## ğŸ”„ Workflow

1. ğŸ•¸ï¸ Run the web scraper to collect data:
   ```bash
   python deep_scraper.py
   ```
2. ğŸ§  Process the scraped data and generate embeddings:
   ```bash
   python modified_embedder.py
   ```
3. ğŸ’¬ Use the Provided Jupyter Notebook:
   ```bash
   run all the cells in the Jupyter Notebook
   ```

---

## ğŸ“ Data Files

The project uses the following data files:

- `srmap_metadata_deep.pkl`: Metadata storage
- `srmap_faiss_deep.index`: FAISS index for similarity search
- `srmap_data_deep.pkl`: Processed data storage

Note: These files are not included in the repository and should be generated during the first run.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“¬ Contact

Vamshi Krishna  
https://github.com/vamshi533
